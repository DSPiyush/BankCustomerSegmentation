{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33c4599-3508-4307-833d-f7d3e815d234",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "260feb96-9da6-4fa8-9843-23d7f406febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import to_date,from_unixtime, unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38635469-2e83-4e79-bf39-1b89c137ae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/24 18:18:55 WARN Utils: Your hostname, myUbuntu resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlo1)\n",
      "23/08/24 18:18:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/24 18:18:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d61dc399-e6eb-4100-9c10-2824ed4cdbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = r\"/home/hadoop/CustomerSegmentation/bank_transactions.csv\"\n",
    "schema = StructType([\n",
    "    StructField(\"TransactionID\", StringType(), True),\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"CustomerDOB\", StringType(), True),\n",
    "    StructField(\"CustGender\", StringType(), True),\n",
    "    StructField(\"CustLocation\", StringType(), True),\n",
    "    StructField(\"CustAccountBalance\", DoubleType(), True),\n",
    "    StructField(\"TransactionDate\", StringType(), True),\n",
    "    StructField(\"TransactionTime\", StringType(), True),\n",
    "    StructField(\"TransactionAmount (INR)\", DoubleType(), True)\n",
    "])\n",
    "sdf = spark.read.option(\"header\", \"true\").schema(schema).csv(csv_file_path)\n",
    "#sdf = sdf.withColumn(\"TransactionDate\", to_date(\"TransactionDate\", \"d/m/yy\")) with Arvind Bhai to convert Dates and times to correct DataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69d06f9e-6fb3-41c7-b24b-70507e767158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "|TransactionID|CustomerID|CustomerDOB|CustGender|CustLocation|CustAccountBalance|TransactionDate|TransactionTime|TransactionAmount (INR)|\n",
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "|           T1|  C5841053|    10/1/94|         F|  JAMSHEDPUR|          17819.05|         2/8/16|         143207|                   25.0|\n",
      "|           T2|  C2142763|     4/4/57|         M|     JHAJJAR|           2270.69|         2/8/16|         141858|                27999.0|\n",
      "|           T3|  C4417068|   26/11/96|         F|      MUMBAI|          17874.44|         2/8/16|         142712|                  459.0|\n",
      "|           T4|  C5342380|    14/9/73|         F|      MUMBAI|         866503.21|         2/8/16|         142714|                 2060.0|\n",
      "|           T5|  C9031234|    24/3/88|         F| NAVI MUMBAI|           6714.43|         2/8/16|         181156|                 1762.5|\n",
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2207d716-4833-4780-afc9-e2f332fe26d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TransactionID: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- CustomerDOB: string (nullable = true)\n",
      " |-- CustGender: string (nullable = true)\n",
      " |-- CustLocation: string (nullable = true)\n",
      " |-- CustAccountBalance: double (nullable = true)\n",
      " |-- TransactionDate: string (nullable = true)\n",
      " |-- TransactionTime: string (nullable = true)\n",
      " |-- TransactionAmount (INR): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "630a3839-6729-45a4-8b24-d6abc763caac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------------+\n",
      "|summary|CustAccountBalance|TransactionAmount (INR)|\n",
      "+-------+------------------+-----------------------+\n",
      "|  count|           1046198|                1048567|\n",
      "|   mean|115403.54005622237|     1574.3350034571092|\n",
      "| stddev| 846485.3806006602|      6574.742978454001|\n",
      "|    min|               0.0|                    0.0|\n",
      "|    max|     1.150354951E8|             1560034.99|\n",
      "+-------+------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = sdf.select([col for col, dtype in sdf.dtypes if dtype == \"double\" or dtype == \"int\"])\n",
    "numeric_columns.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86ea3a79-d25a-4760-ac2c-388cebb44738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping the na values : (1048567,9)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape before dropping the na values : ({sdf.count()},{len(sdf.columns)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bbdf3198-4ec2-4ee3-a539-2f4ad8992771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping the na values : (1044947,9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_cleaned = sdf.na.drop()\n",
    "print(f\"Shape after dropping the na values : ({sdf_cleaned.count()},{len(sdf_cleaned.columns)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5f6703bd-00ba-4ce3-9230-4df73e50a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "|TransactionID|CustomerID|CustomerDOB|CustGender|CustLocation|CustAccountBalance|TransactionDate|TransactionTime|TransactionAmount (INR)|\n",
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "|           T1|  C5841053|    10/1/94|         F|  JAMSHEDPUR|          17819.05|         2/8/16|         143207|                   25.0|\n",
      "|           T2|  C2142763|     4/4/57|         M|     JHAJJAR|           2270.69|         2/8/16|         141858|                27999.0|\n",
      "|           T3|  C4417068|   26/11/96|         F|      MUMBAI|          17874.44|         2/8/16|         142712|                  459.0|\n",
      "|           T4|  C5342380|    14/9/73|         F|      MUMBAI|         866503.21|         2/8/16|         142714|                 2060.0|\n",
      "|           T5|  C9031234|    24/3/88|         F| NAVI MUMBAI|           6714.43|         2/8/16|         181156|                 1762.5|\n",
      "+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c72f3-5dcb-4706-b31a-4bd50c9fcfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40180c-5691-44a5-a643-527b79eaf9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682743f-77b7-4d93-8308-ac0664fa8255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c86f2-0a36-4473-9d4e-a3bb91fdaa51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e4778-4720-47a0-9b48-967843d7e005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "673f7d8e-10aa-4032-b3fb-b2bdf6b4ca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(InvoiceNo)|\n",
      "+----------------+\n",
      "|541909          |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select InvoiceNo from orders where \").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6533d-5ded-4011-8293-78bcf41928a6",
   "metadata": {},
   "source": [
    "### Creating PySpark Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b28c9ce-1a96-475b-ae1c-dacd2d293a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/24 14:40:32 WARN Utils: Your hostname, myUbuntu resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlo1)\n",
      "23/08/24 14:40:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/24 14:40:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a986ec-4adb-42bc-8126-decdc9e420f8",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d6b98ba-a746-4597-bed5-99801880dd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/.local/lib/python3.8/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                          Description  Quantity     InvoiceDate  UnitPrice  CustomerID         Country\n",
       "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6  12/1/2010 8:26       2.55       17850  United Kingdom\n",
       "1    536365     71053                  WHITE METAL LANTERN         6  12/1/2010 8:26       3.39       17850  United Kingdom\n",
       "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8  12/1/2010 8:26       2.75       17850  United Kingdom\n",
       "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6  12/1/2010 8:26       3.39       17850  United Kingdom\n",
       "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6  12/1/2010 8:26       3.39       17850  United Kingdom"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"/home/hadoop/CustomerSegmentation/data.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d47f7-2b7e-4075-ad10-c19b440f03e3",
   "metadata": {},
   "source": [
    "### Understanding the Data\n",
    "\n",
    "#### 1) cat_columns(dataset,thshold=4) \n",
    "\n",
    "This function will be used for finding the categorical column in the data set you provide as an input, this also gives user the facility tochange the **thshold** parameter value to let the function know if ther number of minimum number of category needs to be defined. \n",
    " This function returns a list of Columns which are highly possible to be categorical in Nature\n",
    "#### 2) summarize_data(data)\n",
    "\n",
    "This function will be used to have a basic summary of our dataframe we put it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415d4f4c-78d7-463d-9b13-678b29f477c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have used the thshold as 38 as the value as df['Country'].value_counts().count = 38\n",
    "def cat_columns(dataset,thshold=38):\n",
    "    \"\"\"\n",
    "       This will take Dataset name as the input and return a list of columns \n",
    "       from the same Dataset which are Categorical\n",
    "       \n",
    "       Note: There might be a few mismatches.\n",
    "       \n",
    "    \"\"\" \n",
    "    l = dataset.columns\n",
    "    list_vcount_count = []\n",
    "    for col in l:\n",
    "        list_vcount_count.append(dataset[col].value_counts().count())\n",
    "    list_vcount_count.sort()    \n",
    "    for threshhold in range(len(list_vcount_count)):\n",
    "        diff = list_vcount_count[threshhold+1] - list_vcount_count[threshhold]\n",
    "        if diff > list_vcount_count[threshhold]:\n",
    "            break \n",
    "    #threshhold Value is intact\n",
    "    categorical_columns = []\n",
    "    for col in l:\n",
    "        if dataset[col].value_counts().count() <= threshhold:\n",
    "            categorical_columns.append(col)\n",
    "        elif dataset[col].value_counts().count() > 0 and dataset[col].value_counts().count() <= thshold:\n",
    "            categorical_columns.append(col)\n",
    "    return categorical_columns\n",
    "def summarize_data(data):\n",
    "    print(f\"Head :\\n{data.head()}\")\n",
    "    print(40*\"*\")\n",
    "    print(f\"Shape:\\n{data.shape}\")\n",
    "    print(40*\"*\")\n",
    "    print(f\"All about Exp-Salaries :\\n{data.describe()}\")\n",
    "    print(40*\"*\")\n",
    "    print(f\"Columnns :\\n{data.columns}\")\n",
    "    print(40*\"*\")\n",
    "    for i in cat_columns(data):\n",
    "        print(f\"Value counts for {i}\\n{data[i].value_counts()}\")\n",
    "        print(10*(\"-\"))\n",
    "    print(40*\"*\")\n",
    "    print(f\"Checking if there are any null values :\\n{data.isna().any()}\") # If Null values present fill them/take care of them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a59193-20e7-4429-af1a-b12d19f6d815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head :\n",
      "  InvoiceNo StockCode                          Description  Quantity     InvoiceDate  UnitPrice  CustomerID         Country\n",
      "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6  12/1/2010 8:26       2.55       17850  United Kingdom\n",
      "1    536365     71053                  WHITE METAL LANTERN         6  12/1/2010 8:26       3.39       17850  United Kingdom\n",
      "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8  12/1/2010 8:26       2.75       17850  United Kingdom\n",
      "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6  12/1/2010 8:26       3.39       17850  United Kingdom\n",
      "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6  12/1/2010 8:26       3.39       17850  United Kingdom\n",
      "****************************************\n",
      "Shape:\n",
      "(541909, 8)\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/24 14:40:52 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All about Exp-Salaries :\n",
      "            Quantity      UnitPrice     CustomerID\n",
      "count  541909.000000  541909.000000  406829.000000\n",
      "mean        9.552250       4.611114   15287.690570\n",
      "std       218.081158      96.759853    1713.600303\n",
      "min    -80995.000000  -11062.060000   12346.000000\n",
      "25%         1.000000       1.250000   13952.000000\n",
      "50%         3.000000       2.080000   15152.000000\n",
      "75%        10.000000       4.130000   16791.000000\n",
      "max     80995.000000   38970.000000   18287.000000\n",
      "****************************************\n",
      "Columnns :\n",
      "Index(['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate',\n",
      "       'UnitPrice', 'CustomerID', 'Country'],\n",
      "      dtype='object')\n",
      "****************************************\n",
      "Value counts for Country\n",
      "United Kingdom          495478\n",
      "Germany                   9495\n",
      "France                    8557\n",
      "EIRE                      8196\n",
      "Spain                     2533\n",
      "Netherlands               2371\n",
      "Belgium                   2069\n",
      "Switzerland               2002\n",
      "Portugal                  1519\n",
      "Australia                 1259\n",
      "Norway                    1086\n",
      "Italy                      803\n",
      "Channel Islands            758\n",
      "Finland                    695\n",
      "Cyprus                     622\n",
      "Sweden                     462\n",
      "Unspecified                446\n",
      "Austria                    401\n",
      "Denmark                    389\n",
      "Japan                      358\n",
      "Poland                     341\n",
      "Israel                     297\n",
      "USA                        291\n",
      "Hong Kong                  288\n",
      "Singapore                  229\n",
      "Iceland                    182\n",
      "Canada                     151\n",
      "Greece                     146\n",
      "Malta                      127\n",
      "United Arab Emirates        68\n",
      "European Community          61\n",
      "RSA                         58\n",
      "Lebanon                     45\n",
      "Lithuania                   35\n",
      "Brazil                      32\n",
      "Czech Republic              30\n",
      "Bahrain                     19\n",
      "Saudi Arabia                10\n",
      "Name: Country, dtype: int64\n",
      "----------\n",
      "****************************************\n",
      "Checking if there are any null values :\n",
      "InvoiceNo      False\n",
      "StockCode      False\n",
      "Description     True\n",
      "Quantity       False\n",
      "InvoiceDate    False\n",
      "UnitPrice      False\n",
      "CustomerID      True\n",
      "Country        False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "summarize_data(df)\n",
    "#The Warning can be safeley ignored - Per the StackOverFlow\n",
    "#You can safely ignore it, if you are not interested in seeing the sql schema logs.\n",
    "#Otherwise, you might want to set the property to a higher value, but it might affect the performance of your job:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e20c432-2995-4d8f-a1ab-e9e14954674c",
   "metadata": {},
   "source": [
    "### This dataframe contains 8 variables that correspond to:\n",
    "\n",
    "InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "\n",
    "StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "\n",
    "Description: Product (item) name. Nominal.\n",
    "\n",
    "Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "\n",
    "InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "\n",
    "UnitPrice: Unit price. Numeric, Product price per unit in sterling(currency).\n",
    "\n",
    "CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "\n",
    "Country: Country name. Nominal, the name of the country where each customer resides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e822d86-7e10-4fed-8e79-d549a8090ef8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Lets do some analysis on the Invoice Number and take out all the Cancelled Invoices\n",
    "\n",
    "#### We shall use spark SQL to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77a02052-db8e-4663-bcc2-5501ae13d1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvoiceNo       object\n",
       "StockCode       object\n",
       "Description     object\n",
       "Quantity         int32\n",
       "InvoiceDate     object\n",
       "UnitPrice      float64\n",
       "CustomerID       int32\n",
       "Country         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131c5ea7-7d2a-44dd-9078-aac5364c6a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c68b4-9f9d-4e50-8440-9194e9bec08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('InvoiceNo', StringType() nullable=False),\n",
    "    StructField('StockCode', StringType(), nullable=False),\n",
    "    StructField('Description', StringType(), nullable=False),\n",
    "\n",
    "\n",
    "                \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc82576-cdea-4644-86c2-1959eac44fb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:1276\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1275\u001b[0m     )\n\u001b[0;32m-> 1276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:1318\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1316\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1318\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:962\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    959\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 962\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    964\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:836\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    834\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    835\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m--> 836\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:839\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    834\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    835\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    836\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[1;32m    837\u001b[0m     _merge_type,\n\u001b[1;32m    838\u001b[0m     (\n\u001b[0;32m--> 839\u001b[0m         \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[1;32m    847\u001b[0m     ),\n\u001b[1;32m    848\u001b[0m )\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:1566\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(row\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[1;32m   1568\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "sdf = spark.createDataFrame(df, schema= schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13d310-9637-4fa4-bc81-70c9f35ad6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
